{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"esercizio1 .ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yX01LhO_OjSj"},"source":["<h1>Importing resources<h1>\n","\n","\n","\n","1.   defs.csv\n","2.   stop words file\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFCE28TlOduZ","executionInfo":{"status":"ok","timestamp":1636914445511,"user_tz":-60,"elapsed":3140,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE29R-M02GMpjyLwQi8WfZcyWkLCX1CVXfyU1glR0=s64","userId":"13351824108178577276"}},"outputId":"24330a22-5c09-4ebb-8256-6523a14de8d8"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string #per punteggiatura\n","from nltk.corpus import wordnet as wn\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","\n","from numpy import mean\n","import pandas as pd\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4PWNF4hNrcE","executionInfo":{"status":"ok","timestamp":1636914445514,"user_tz":-60,"elapsed":30,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE29R-M02GMpjyLwQi8WfZcyWkLCX1CVXfyU1glR0=s64","userId":"13351824108178577276"}},"outputId":"4896625e-71f2-49d3-8646-6a4f2cae6bee"},"source":["df = pd.read_csv (r'defs.csv')\n","\n","\n","def get_definition_for_concept(concept):\n","    concept_array= df[[concept]].to_numpy()\n","    \n","    concept_list=[]\n","    for arr in concept_array:\n","      concept_list.append(str(arr[0]))\n","    return concept_list\n","\n","courage_definitions=get_definition_for_concept(\"Courage\") #astratto generico\n","\n","paper_definitions=get_definition_for_concept(\"Paper\")\n","apprehension_definitions=get_definition_for_concept(\"Apprehension\")\n","\n","sharpener_definitions=get_definition_for_concept(\"Sharpener\")\n","\n","print(courage_definitions)\n","print(type(courage_definitions))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['nan', 'Ability to face our own fears and do something that scars us or makes us unpleasent', 'the ability to face thing without fear ', 'Inner strength thaht allow you to face particular situations', 'Ability to control the fear', 'nan', 'Ability to avoid fear and to take on risky actions', 'nan', 'Being able to do something fearful', 'the ability to do something despite being frightened.', 'nan', 'Feeling that allows us to face situations considered dangerous', 'the ability to do something that may be scary', 'is the ability to make drastic choices', 'ability to overcome fear', 'characteristic of a person who taking a risk', 'the quality of being able to do things which are generally dangerous or scaring', 'behavior typical of a hero', 'Ability to face difficult situations', 'ability to do something most people fear', 'Strength of mind that allows you to face difficult situations', \"Ability to face one's fears\", 'The ability to not be blocked by fear', 'emotion that allows someone to go beyond its expectations', 'Ability to perform a dangerous act despite the fear', 'nan', 'Ability to resist fear and not being scared in situations that provoke fear', 'ability to overcome fears', 'The ability of overcome your fear and face difficult situations', 'nan']\n","<class 'list'>\n"]}]},{"cell_type":"code","metadata":{"id":"ENaa1V1RQeQE","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1636914445517,"user_tz":-60,"elapsed":28,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE29R-M02GMpjyLwQi8WfZcyWkLCX1CVXfyU1glR0=s64","userId":"13351824108178577276"}},"outputId":"be4f157e-acfa-476f-d517-7db303b1c561"},"source":["stop_words_list = []\n","lemmatizer = WordNetLemmatizer() \n","\n","'''\n","Lemmatization is the process of converting a word to its base form. \n","The difference between stemming and lemmatization is, lemmatization considers \n","the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, \n","often leading to incorrect meanings and spelling errors.\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nLemmatization is the process of converting a word to its base form. \\nThe difference between stemming and lemmatization is, lemmatization considers \\nthe context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, \\noften leading to incorrect meanings and spelling errors.\\n'"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"lupALpT7r_a4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636914445518,"user_tz":-60,"elapsed":26,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE29R-M02GMpjyLwQi8WfZcyWkLCX1CVXfyU1glR0=s64","userId":"13351824108178577276"}},"outputId":"bc69b7c7-da4d-4b14-df25-86efb2b7e79f"},"source":[" nltk.download('tagsets') \n","#decommentare per vedere tagset completo nltk \n","#(adverb(RB),adverb comparative(RBR), adverbe superlative(RBS), adj(JJ), adj superlative(JJS) ecc.. )\n","#nltk.help.upenn_tagset()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"3CmdA8yrubwn"},"source":["Data una parola word, la tokenizzo con nltk.word_tokenize, che ritorna una lista di token.\n","Successivamente nltk.pos_tag ritorna una lista di tuple (word,tag), di cui \n","io voglio solo il tag del primo elemento (ce ne è uno solo trattandosi di una sola parola e non di una frase), quindi vi accedo con [0][1] (cioè il secondo elemento del primo elemento della lista).\n","In seguito ritorno il tag wordnet in base al tag nltk\n"]},{"cell_type":"code","metadata":{"id":"k1Rq_hddp44c"},"source":["# Ritorna il POS TAG di una parola\n","def get_wordnet_pos(word):\n","    '''\n","    nltk.pos_tag(nltk.word_tokenize(word)) ha questo risultato\n","\n","    [('Ability', 'NN')]\n","    [('to', 'TO')]\n","    [('face', 'NN')]\n","        ....\n","        ....\n","        ....\n","    Volgio solo determinare il POS TAG di una parola, quindi: nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n","    '''\n","    treebank_tag = nltk.pos_tag(nltk.word_tokenize(word))[0][1] \n","    if treebank_tag.startswith('J'):# JJ: adjective or numeral, ordinal JJR: adjective, comparative\n","        return wn.ADJ\n","    elif treebank_tag.startswith('V'): #VB: verb, base form VBN: verb, past participle VBP: verb, present tense, not 3rd person singular VBZ: verb, present tense, 3rd person singular \n","        return wn.VERB\n","    elif treebank_tag.startswith('N'): #NN: noun, common, singular or mass NNP: noun, proper, singular NNPS: noun, proper, plural NNS: noun, common, plural\n","        return wn.NOUN\n","    elif treebank_tag.startswith('R'): # RB: adverb RBR: adverb, comparative RBR: adverb, comparative RP: particle\n","        return wn.ADV\n","    else:\n","        return ''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PKdg_CbLqK6"},"source":["# Ritorna una lista con le stop words\n","def get_stop_words():\n","    if len(stop_words_list) == 0:\n","       \n","        f = open(\"stop_words_FULL.txt\", \"r\") \n","        for x in f:\n","            stop_words_list.append(x)\n","    # aggiunge l'apostrofo alla lista delle spot words in modo da ignorarlo\n","    return stop_words_list + ['’']\n","\n","\n","# Da una frase ritorna una lista con le singole parole (lemmi) rimuovendo le parole inutili (stop words..)\n","def get_list_of_gains_words(sentence):\n","    list_words_lemma = []\n","    # aus_list_words = sentence.split()\n","    aus_list_words = word_tokenize(sentence) #tokenizziamo la frase\n","    stop_words_list = get_stop_words() #prendo le stop words  dal file\n","    #per ogni parola nella lista dei token\n","    for w in aus_list_words:\n","        if w.lower() not in stop_words_list:#se la parola non è una stopword\n","            pos_tag = get_wordnet_pos(w) #prendo il post_tag di quella parola\n","            if pos_tag != '':\n","                #lemmatize può prendere come paramentro anche il pos tag per restringere il campo di ricerca del lemma\n","                list_words_lemma.append(lemmatizer.lemmatize(w.lower(), pos_tag)) \n","\n","    return list_words_lemma\n","\n","# Ritorna la lista delle parole della signature di un synset lemmatizzata e senza stop words : gloss(definizione) + examples\n","def get_signature_of_synset(synset):\n","    # Gloss\n","    signature = get_list_of_gains_words(synset.definition()) #definizione senza stop words e con i lemmi\n","    # Examples\n","    for ex in synset.examples():\n","        list_ex = get_list_of_gains_words(ex) #esempio senza stop words e con i lemmi\n","        signature.extend(list_ex) # aggiunge l'esempio alla signature\n","\n","    return signature\n","\n","\n","# Lemmatizzazione e rimozione stop word\n","def preprocessing():\n","    for i in range(len(courage_definitions)):\n","        courage_definitions[i] = bag_of_words(get_list_of_gains_words(courage_definitions[i]))\n","    \n","    for i in range(len(paper_definitions)):\n","        paper_definitions[i] = bag_of_words(get_list_of_gains_words(paper_definitions[i]))\n","    for i in range(len(sharpener_definitions)):\n","        sharpener_definitions[i] = bag_of_words(get_list_of_gains_words(sharpener_definitions[i]))\n","    for i in range(len(apprehension_definitions)):\n","        apprehension_definitions[i] = bag_of_words(get_list_of_gains_words(apprehension_definitions[i]))\n","    \n","\n","# Restitusice l'insieme delle parole utilizzate nelle definizioni di\n","# un concetto (paper, sharpener, courage, apprehension)\n","def word_set(definizioni):\n","    words = set()\n","\n","    for defin in definizioni:\n","        words.update(bag_of_words(defin))\n","        #for word in defin:\n","        #words.add(word)\n","\n","    return words\n","\n","\n","# Data una lista di parole restituisce l'insieme delle parole (set) togliendo le parole ripetute\n","def bag_of_words(sentence):\n","    bag = set()\n","    for word in sentence:\n","        bag.add(word)\n","    return bag\n","\n","\n","# Dato un insieme di definizioni ne calcola il valore di similarità\n","def definitions_similarity(definitions):\n","    similarity = 0\n","    for i in range(len(definitions)):\n","        for j in range(len(definitions)):\n","            if i != j:\n","                similarity += set_similarity(definitions[i], definitions[j])\n","\n","    n = len(definitions) * len(definitions) - len(definitions)\n","    return similarity / n\n","\n","\n","# Restituisce la similarità tra due definizione mediante cardinalità dell’intersezione dei termini\n","# normalizzata su lunghezza minima tra le due)\n","\"\"\"\n","es prof slide 19 lez 1.2\n","abcde\n","abdef     b,e   2/4=0,5 dove 4 è la lunghezza media delle definizioni di una parola ( ma noi usiamo la lunghezza minima)\n","bef\n","\"\"\"\n","def set_similarity(def1: set, def2: set):#gli passiamo delle definizioni che trasformiamo in set\n","    return len(def1.intersection(def2)) / min(len(def1), len(def2))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"epIbGUh6x0va","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636914448047,"user_tz":-60,"elapsed":2549,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE29R-M02GMpjyLwQi8WfZcyWkLCX1CVXfyU1glR0=s64","userId":"13351824108178577276"}},"outputId":"33ad3053-dcd4-4503-90b0-461e8572c0bf"},"source":["#\n","# MAIN\n","#\n","\n","preprocessing()\n","\n","print(\"Astratto - Generico: \", definitions_similarity(courage_definitions))\n","print(\"Astratto - Specifico: \", definitions_similarity(apprehension_definitions))\n","print(\"Concreto - Generico: \", definitions_similarity(paper_definitions))\n","print(\"Concreto - Specifico: \", definitions_similarity(sharpener_definitions))\n","print()\n","\n","#########################################################################################\n","#########################################################################################\n","#                   UTILIZZO DI WORDNET                                                 #\n","# Viene calcolata la similarità tra l'insieme delle parole utilizzate nelle             #\n","# definizioni di ciascun concetto e l'insieme delle parole utilizzate nella glossa e    #\n","# negli esempi del corrispondente WordNet synset                                        #\n","#########################################################################################\n","#########################################################################################\n","\n","# Prendo gli insiemi delle parole utilizzate nelle gloss e nelle definizioni (signature)\n","# dei termini paper, sharpener, courage, apprehension\n","print(\"Con utilizzo di gloss e esempi di WordNet:\")\n","paper_synset = wn.synsets('paper')[0] #prende il primo synset di wordnet per ogni concetto\n","sharpener_synset = wn.synsets('sharpener')[0]\n","courage_synset = wn.synsets('courage')[0]\n","apprehension_synset = wn.synsets('apprehension')[0]\n","\n","paper_signature = set(get_signature_of_synset(paper_synset))\n","sharpener_signature = set(get_signature_of_synset(sharpener_synset))\n","courage_signature = set(get_signature_of_synset(courage_synset))\n","apprehension_signature = set(get_signature_of_synset(apprehension_synset))\n","\n","# Prendo gli insiemi delle parole utilizzate nelle NOSTRE definizioni\n","# dei termini paper, sharpener, courage, apprehension\n","bulding_words_set = word_set(paper_definitions)\n","sharpener_words_set = word_set(sharpener_definitions)\n","courage_words_set = word_set(courage_definitions)\n","apprehension_words_set = word_set(apprehension_definitions)\n","\n","# Calcolo similarità tra definizioni ( come set di parole depuarte e lemmatizzate) nostre e quelle di WordNet  (cardinalità dell’intersezione dei termini\n","# normalizzata su lunghezza minima tra le due)\n","astratto_generico = len(courage_words_set.intersection(courage_signature)) / min(len(courage_words_set),\n","                                                                                  len(courage_signature))\n","astratto_specifico = len(apprehension_words_set.intersection(apprehension_signature)) / min(len(apprehension_words_set),\n","                                                                                        len(apprehension_signature))\n","concreto_generico = len(bulding_words_set.intersection(paper_signature)) / min(len(bulding_words_set),\n","                                                                                  len(paper_signature))\n","concreto_specifico = len(sharpener_words_set.intersection(sharpener_signature)) / min(len(sharpener_words_set),\n","                                                                                    len(sharpener_signature))\n","# Stampa dei risultati di similarità\n","print(\"Astratto - Generico: \", astratto_generico)\n","print(\"Astratto - Specifico: \", astratto_specifico)\n","print(\"Concreto - Generico: \", concreto_generico)\n","print(\"Concreto - Specifico: \", concreto_specifico)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Astratto - Generico:  0.2025916803503011\n","Astratto - Specifico:  0.12072796934865905\n","Concreto - Generico:  0.2811384783798581\n","Concreto - Specifico:  0.37766283524904154\n","\n","Con utilizzo di gloss e esempi di WordNet:\n","Astratto - Generico:  0.375\n","Astratto - Specifico:  0.375\n","Concreto - Generico:  0.5\n","Concreto - Specifico:  0.5\n"]}]},{"cell_type":"markdown","metadata":{"id":"noXCigQABSaL"},"source":["#Esercizio 1.2\n","Stampiamo le 3 parole più usate dalle definizioni di ogni concetto\n","per provare a dare una spiegazione ai valori di similarità"]},{"cell_type":"code","metadata":{"id":"4N9s5UdfATqC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636914448048,"user_tz":-60,"elapsed":16,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiE29R-M02GMpjyLwQi8WfZcyWkLCX1CVXfyU1glR0=s64","userId":"13351824108178577276"}},"outputId":"2e115b83-72ff-4061-b2fc-c9dfabf8cc22"},"source":["# Data la lista delle parole, la ritorna ordinata in base alla frequenza\n","def get_ordered_list_words(definizioni):\n","    common_words = []\n","   \n","    fdist1 = FreqDist(token.lower() for word in definizioni \n","                      for token in word )\n","    for tupla in fdist1.most_common():\n","      lista= list (tupla)\n","      common_words.append(lista[0])\n","    return common_words\n","\n","list_concepts=[courage_definitions,paper_definitions,apprehension_definitions,sharpener_definitions]\n","\n","# Per ogni concetto da trovare\n","for c in list_concepts :\n","  # Si prendono tutti i termini a partire dalle definizioni scritte per il concetto, ordinati per frequenza\n","  concept_words = get_ordered_list_words(c)\n","  print(concept_words[:3])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['ability', 'fear', 'face']\n","['material', 'write', 'use']\n","['feel', 'anxiety', 'state']\n","['pencil', 'sharpen', 'use']\n"]}]}]}